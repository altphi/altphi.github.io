
How GPT works...

Say we have 3 files, one line each.
- "Cats chase mice."
- "Dogs chase cats."
- "Mice eat cheese."

1. combine texts into one training stream
`Cats chase mice. Dogs chase cats. Mice eat cheese.`

2. tokenize the text (long words often broken into multiple tokens, read on BPE later)
```json
["Cats", "chase", "mice", ".",
 "Dogs", "chase", "cats", ".",
 "Mice", "eat", "cheese", "."]
 ```

3. embedding layer
a. form _input_ -> _target_ pairs
- `Cats` -> `chase`
- `Cats chase` -> `mice`
- `Cats chase mice` -> `.`
- `Cats chase mice.` -> `Dogs`
- `Cats chase mice. Dogs` -> `chase`
etc...

Each of these is called a 'prediction task.'
Each token is vectorized... ???

...

-> embeddings layer
-> attention layer
-> neural network layer
-> prediction



types of models
Encoder-only (e.g. BERT) -- used for understanding/embedding only
Decoder-only (e.g. GPT)  -- used for next-token prediction / generation (aka GenAI)
Encoder-Deocder (e.g. T5) -- used for translation

---

Terms:
- `token` - ...
- `a vocabulary` - finite list of tokens, each with unique integer ID, fixed
  before training and never changes during training
- `d_model` - the number of dimensions for a model, a design choice, the larger
  the dimension the more information can be represented, and the more expensive
  the compute.  Early models (GPT-2) were around 768, GPT-3 12,228, and current
  models are much larger.
- `embedding matrix` - learned lookup table, a matrix that is `vocab length x
  d_model`, each row corresponds to one token and that row is called an
  embedding vector
- `embedding vector` - row in the embedding matrix for a single token


---

Things to read...
[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
[Attention is All You Need](https://arxiv.org/abs/1706.03762)


Other notes...
RNNs and LSTMs are the old ways replaced by Transformers

#dailies
#wip
